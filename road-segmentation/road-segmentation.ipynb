{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6f0a46-9ce8-4d01-a9d8-e5fbffc70f25",
   "metadata": {},
   "source": [
    "## Road Segmentation \n",
    "using fully convolutional network <br>\n",
    "and pretrained VGG-16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b33244-3d64-4f11-9573-e8f4d146557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c06e06ec-1567-4ca1-b7af-d2a6967b07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_dir = glob.glob(\"./data/data_road/training/image_2/*.png\")\n",
    "train_gt_dir = glob.glob(\"./data/data_road/training/gt_image_2/*.png\")\n",
    "test_dir = glob.glob(\"./data/data_road/testing/image_2/*.png\")\n",
    "\n",
    "image_dataset_path = \"./data/data_road/training/image_2/\"\n",
    "mask_dataset_path = \"./data/data_road/training/gt_image_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe2a196-851a-4217-9e31-00a7622fcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, transforms):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.target_paths[idx])\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "        return (image, mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a694e1-f1c4-4629-ad3c-bb4e3d7a77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = sorted(list(paths.list_images(image_dataset_path)))\n",
    "mask_path = sorted(list(paths.list_images(mask_dataset_path)))\n",
    "\n",
    "split = train_test_split(im_path, mask_path, test_size=0.2, random_state=42)\n",
    "\n",
    "(train_im, test_im) = split[:2]\n",
    "(train_mask, test_mask) = split[2:]\n",
    "\n",
    "# write the testing image paths to disk so that we can use then\n",
    "# when evaluating/testing our model\n",
    "#print(\"[INFO] saving testing image paths...\")\n",
    "#f = open(config.TEST_PATHS, \"w\")\n",
    "#f.write(\"\\n\".join(testImages))\n",
    "#f.close()\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToPILImage(), \n",
    "                                 transforms.Resize((224,224)),\n",
    "                                 transforms.functional.rgb_to_grayscale,\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "train_dataset = RoadSegmentationDataset(train_im, train_mask, transforms)\n",
    "test_dataset = RoadSegmentationDataset(test_im, test_mask, transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c95dd63-c616-4f3f-8abd-c5fb6e46212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2(image, mask):\n",
    "    image = image.permute(1,2,0)\n",
    "    mask = mask.permute(1,2,0)\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32d73cf7-9a50-42a4-8a0a-e6f080041207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Batch Shape: torch.Size([32, 1, 224, 224])\n",
      "Labels Batch Shape: torch.Size([32, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# first batch\n",
    "train_features, train_mask = next(iter(train_loader))\n",
    "print(f\"Features Batch Shape: {train_features.size()}\")\n",
    "print(f\"Labels Batch Shape: {train_mask.size()}\")\n",
    "img = train_features[2].squeeze()\n",
    "mask = train_mask[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c3f344-efa6-490d-a6db-200ef3cf3ad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mplot_2\u001b[0;34m(image, mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_2\u001b[39m(image, mask):\n\u001b[0;32m----> 2\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "plot_2(img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bbbb6-ab8f-4b74-bf1d-a44a09570efd",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acc9ac4e-d3c9-46d7-b9b4-4063736c2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34c647ff-f32b-4b4d-8362-afa882596d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf9a9a-418b-4c82-b65c-e0474ab9238e",
   "metadata": {},
   "source": [
    "Freeze convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1049cda6-e343-40f2-9163-d42536869d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9092e29d-36df-4376-80d7-5804bfe5ac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN32(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(4096, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ConvTranspose2d(21, 21, kernel_size=(224, 224), stride=(32, 32))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the classifier of vgg16 to concolutional layer and upsample\n",
    "\n",
    "class FCN32(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(FCN32, self).__init__()\n",
    "    self.features = vgg16.features\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Conv2d(512, 4096, 7),\n",
    "      nn.ReLU(inplace=True),\n",
    "      #nn.Dropout2d(),\n",
    "      nn.Conv2d(4096, 4096, 1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      #nn.Dropout2d(),\n",
    "      nn.Conv2d(4096, 21, 1),\n",
    "      nn.ConvTranspose2d(21, 21, 224, stride=32)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "fcn = FCN32()\n",
    "fcn.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fe1eb74-d651-464c-89b9-051c6661c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE parameters requires gradients: False\n",
      "CLASSIFIER parameters requires gradients: True\n",
      "Transferred FEATURE parameters from VGG16 to FCN: True\n",
      "Number of params transfered: 14714688\n",
      "Number of params left in network to be updated: 141759530\n"
     ]
    }
   ],
   "source": [
    "print('FEATURE parameters requires gradients: {}'.format(any([param.requires_grad for param in fcn.features.parameters()])))\n",
    "print('CLASSIFIER parameters requires gradients: {}'.format(all([param.requires_grad for param in fcn.classifier.parameters()])))\n",
    "print('Transferred FEATURE parameters from VGG16 to FCN: {}'.format(list(fcn.features.parameters()) == list(vgg16.features.parameters())))\n",
    "print('Number of params transfered: {}'.format(sum(p.numel() for p in fcn.features.parameters())))\n",
    "print('Number of params left in network to be updated: {}'.format(sum(p.numel() for p in fcn.classifier.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6c0fd0-23b3-4bc4-9ab6-f90b1dc3fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(fcn.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0e2bdb4-c6da-4786-a2da-44e85fd20c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_iou(conf_mat, multiplier=1.0):\n",
    "    cm = conf_mat.copy()\n",
    "    np.fill_diagonal(cm, np.diag(cm) * multiplier)\n",
    "    inter = np.diag(cm)\n",
    "    gt_set = cm.sum(axis=1)\n",
    "    pred_set = cm.sum(axis=0)\n",
    "    union_set =  gt_set + pred_set - inter\n",
    "    iou = inter.astype(float) / union_set\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33986c3e-e7db-48b7-a3ed-5ffb4dc12025",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[32, 1, 224, 224] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass for a batch\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mFCN32.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     21\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.nnEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[32, 1, 224, 224] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "best_loss = 1000000000\n",
    "j = 0\n",
    "for epoch in range(15):\n",
    "    val_running_loss = 0\n",
    "    iou_running_score = 0\n",
    "    dice_running_score = 0\n",
    "    fcn.train()\n",
    "    for i, dat in enumerate(train_loader):\n",
    "        j += 1\n",
    "        # Get image, label pair\n",
    "        inputs, labels = dat\n",
    "\n",
    "        # Using GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Set parameter gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for a batch\n",
    "        outputs = fcn(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training loss\n",
    "        print('[{}, {}] training loss:{}'.format(epoch+1, i+1, loss.item()))\n",
    "\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, dat1 in enumerate(test_loader):\n",
    "            # Get image, label pair\n",
    "            inputs1, labels1 = dat1\n",
    "\n",
    "            # Using GPU\n",
    "            inputs1 = inputs1.to(device)\n",
    "            labels1 = labels1.to(device)\n",
    "\n",
    "            # Predicting segmentation for val inputs\n",
    "            outputs1 = fcn(inputs1)\n",
    "\n",
    "            # Compute CE loss and aggregate it\n",
    "            loss1 = criterion(outputs1, labels1)\n",
    "            val_running_loss += loss1\n",
    "\n",
    "            # Reshaping prediction segmentations and actual segmentations for iou and dice score\n",
    "            preds = torch.argmax(outputs1, dim=1).detach().cpu().numpy()\n",
    "            gt = labels1.detach().cpu().numpy()\n",
    "\n",
    "            # Compute confusion matrix\n",
    "            conf_mat = confusion_matrix(y_pred=preds.flatten(), y_true=gt.flatten(), labels=list(range(21)))\n",
    "\n",
    "            # Computing iou and dice scores and aggregating them\n",
    "            iou_score = get_mean_iou(conf_mat=conf_mat)\n",
    "            iou_running_score += iou_score\n",
    "            dice_score = get_mean_iou(conf_mat=conf_mat, multiplier=2.0)\n",
    "            dice_running_score += dice_score\n",
    "\n",
    "        # Averaging loss and scores\n",
    "        avg_val_loss = float(val_running_loss)/(k+1)\n",
    "        avg_iou_score = float(iou_running_score)/(k+1)\n",
    "        avg_dice_score = float(dice_running_score)/(k+1)\n",
    "\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(fcn.state_dict(), '/content/best_model_fcn16.pth.tar')\n",
    "\n",
    "        # Visualizations for batch wise metrics\n",
    "        print('epoch {}, validation loss: {}, iou score: {}, dice score: {}'.format(epoch+1, avg_val_loss, avg_iou_score, avg_dice_score))\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68be1c8-da6c-4fb9-a230-9049b2010494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
